{"backend_state":"running","connection_file":"/projects/6a848d56-5da1-43fb-bfc9-9172977ceeb4/.local/share/jupyter/runtime/kernel-b2e777de-d0aa-4303-a435-173b5f0ce1b3.json","kernel":"nlp_env","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"interpreter":{"hash":"335ee12212264728feb72f243af72c5a8ea26c832f07e1f651ce9e17c7ceae23"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1656599402298,"exec_count":2,"id":"67ee32","input":"# We will read the contents of the Wikipedia article \"Global_warming\" as an example, please feel free to use your own! You can use the url below:\nurl = 'https://en.wikipedia.org/wiki/Barack_Obama' # you can change this to use other sites as well.\n\n# We can open the page using \"urllib.request.urlopen\" then read it using \".read()\"\nsource = urllib.request.urlopen(url).read()\n\n# Beautiful Soup is a Python library for pulling data out of HTML and XML files.\n# you may need to install a parser library --> \"!pip3 install lxml\"\n# Parsing the data/creating BeautifulSoup object\n\nsoup = bs.BeautifulSoup(source,\"html.parser\")","kernel":"nlp_env","pos":5,"start":1656599401246,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599402990,"exec_count":3,"id":"a3e276","input":"# Fetching the data\ntext = \"\"\nfor paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n    text += paragraph.text","kernel":"nlp_env","pos":6,"start":1656599402963,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599405610,"exec_count":4,"id":"8559fa","input":"text[:100]","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"'\\nBarack Hussein Obama II (/bəˈrɑːk huːˈseɪn oʊˈbɑːmə/ (listen) bə-RAHK hoo-SAYN oh-BAH-mə;[1] born A'"},"exec_count":4}},"pos":7,"start":1656599405607,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599410581,"exec_count":5,"id":"b579be","input":"# Preprocessing the data\n\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # [0-9]* --> Matches zero or more repetitions of any digit from 0 to 9\ntext = re.sub(r'\\W^.?!',' ',text) # \\W --> Matches any character which is not a word character except (.?!)\ntext = re.sub(r'\\d',' ',text) # \\d --> Matches any decimal digit\ntext = re.sub(r'\\s+',' ',text) # \\s --> Matches any characters that are considered whitespace (Ex: [\\t\\n\\r\\f\\v].)\n\ntext = text.lower() #everything to lowercase","kernel":"nlp_env","pos":10,"start":1656599410577,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599413154,"exec_count":6,"id":"336033","input":"text[:100]","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"' barack hussein obama ii (/bəˈrɑːk huːˈseɪn oʊˈbɑːmə/ (listen) bə-rahk hoo-sayn oh-bah-mə; born augu'"},"exec_count":6}},"pos":11,"start":1656599413151,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599432637,"exec_count":8,"id":"07440d","input":"# Install NLTK - pip install nltk\nimport nltk\nnltk.download('wordnet')\nnltk.download('punkt')","kernel":"nlp_env","output":{"0":{"name":"stderr","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"},"1":{"data":{"text/plain":"True"},"exec_count":8}},"pos":2,"start":1656599432025,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599436326,"exec_count":9,"id":"4544a5","input":"import urllib\nimport bs4 as bs\nimport re","kernel":"nlp_env","pos":4,"start":1656599436320,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599440174,"exec_count":10,"id":"83de29","input":"'''\nYour code here: Tokenize the words from the data and set it to a variable called words.\nHint: how to this might be on the very home page of NLTK!\n'''\nwords = nltk.word_tokenize(text)","kernel":"nlp_env","pos":13,"start":1656599440088,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599443083,"exec_count":11,"id":"11f34a","input":"print(words[:10])","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"['barack', 'hussein', 'obama', 'ii', '(', '/bəˈrɑːk', 'huːˈseɪn', 'oʊˈbɑːmə/', '(', 'listen']\n"}},"pos":14,"start":1656599443076,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599448123,"exec_count":12,"id":"ee9a53","input":"'''\nYour code here: Tokenize the sentences from the data  and set it to a variable called sentences.\nHint: try googling how to tokenize sentences in NLTK!\n'''\nsentences = nltk.sent_tokenize(text)","kernel":"nlp_env","pos":15,"start":1656599448103,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599450785,"exec_count":13,"id":"39e178","input":"print(sentences[:10])","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"[' barack hussein obama ii (/bəˈrɑːk huːˈseɪn oʊˈbɑːmə/ (listen) bə-rahk hoo-sayn oh-bah-mə; born august , ) is an american politician who served as the th president of the united states from to .', 'he was the first african-american president of the united states.', 'a member of the democratic party, he previously served as a u.s. senator from illinois from to and as an illinois state senator from to .', 'obama was born in honolulu, hawaii.', 'after graduating from columbia university in , he worked as a community organizer in chicago.', 'in , he enrolled in harvard law school, where he was the first black president of the harvard law review.', 'after graduating, he became a civil rights attorney and an academic, teaching constitutional law at the university of chicago law school from to .', 'turning to elective politics, he represented the th district in the illinois senate from until , when he ran for the u.s. senate.', 'obama received national attention in with his march senate primary win, his well-received july democratic national convention keynote address, and his landslide november election to the senate.', 'in , a year after beginning his campaign, and after a close primary campaign against hillary clinton, he was nominated by the democratic party for president.']\n"}},"pos":16,"start":1656599450778,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599455619,"exec_count":14,"id":"72ad1e","input":"'''\ndefine a function called \"remove_stopwords\" that takes in a list of the sentences of the text and returns one that doesn't have any stopwords.\n'''\ndef remove_stopwords(sentences):\n\n    ### Some code goes here. Hint: You may have to look up how to remove stopwords in NLTK if you get stuck. ###\n    for i in range(len(sentences)):\n\n        words = []\n\n        word_list = nltk.word_tokenize(sentences[i])\n\n        for word in word_list:\n            if word not in stopwords.words('english'):\n                words.append(word)\n\n        sentences[i] = ' '.join(words)\n\n    return sentences\n\n###Then actually apply your function###\nsentences = remove_stopwords(sentences)\nprint(sentences[:10]) #Check if it worked correctly. Are all stopwords removed?","kernel":"nlp_env","output":{"0":{"ename":"NameError","evalue":"name 'stopwords' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentences\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m###Then actually apply your function###\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43mremove_stopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentences[:\u001b[38;5;241m10\u001b[39m])\n","Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     11\u001b[0m word_list \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(sentences[i])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_list:\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstopwords\u001b[49m\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     15\u001b[0m         words\u001b[38;5;241m.\u001b[39mappend(word)\n\u001b[1;32m     17\u001b[0m sentences[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n","\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"]}},"pos":19,"scrolled":true,"start":1656599455588,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599459106,"exec_count":15,"id":"eee1d8","input":"'''\ndefine a function called \"remove_punctuation\" that removes punctuation from the sentences.\n'''\ndef remove_punctuation(sentences):\n\n    ### Some code goes here. Hint: Try looking up how to remove stopwords in NLTK if you get stuck. ###\n    for i in range(len(sentences)):\n        words = []\n\n        word_list = nltk.word_tokenize(sentences[i])\n\n        for word in word_list:\n            if word not in \",.?!()\":\n                words.append(word)\n\n        sentences[i] = ' '.join(words)\n\n    return sentences\nsentences = remove_punctuation(sentences)\nprint(sentences[:10]) #eliminating all punctuation.","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"['barack hussein obama ii /bəˈrɑːk huːˈseɪn oʊˈbɑːmə/ listen bə-rahk hoo-sayn oh-bah-mə ; born august is an american politician who served as the th president of the united states from to', 'he was the first african-american president of the united states', 'a member of the democratic party he previously served as a u.s. senator from illinois from to and as an illinois state senator from to', 'obama was born in honolulu hawaii', 'after graduating from columbia university in he worked as a community organizer in chicago', 'in he enrolled in harvard law school where he was the first black president of the harvard law review', 'after graduating he became a civil rights attorney and an academic teaching constitutional law at the university of chicago law school from to', 'turning to elective politics he represented the th district in the illinois senate from until when he ran for the u.s. senate', 'obama received national attention in with his march senate primary win his well-received july democratic national convention keynote address and his landslide november election to the senate', 'in a year after beginning his campaign and after a close primary campaign against hillary clinton he was nominated by the democratic party for president']\n"}},"pos":20,"start":1656599459011,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599463006,"exec_count":16,"id":"7107d4","input":"from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n# try each of the words below\nstemmer.stem('troubled')\nstemmer.stem('trouble')\nstemmer.stem('troubling')\nstemmer.stem('troubles')","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"'troubl'"},"exec_count":16}},"pos":22,"start":1656599463003,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599466817,"exec_count":17,"id":"41227d","input":"'''\nYour code here:\nDefine a function called \"stem_sentences\" that takes in a list of sentences and returns a list of stemmed sentences.\n'''\ndef stem_sentences(sentences):\n    ### Some code goes here. Hint: Try looking up how to stem words in NLTK if you get stuck (or simply use the example above and run stemmer in a loop!).\n    for i in range(len(sentences)):\n        words = []\n\n        word_list = nltk.word_tokenize(sentences[i])\n\n        for word in word_list:\n            words.append(stemmer.stem(word))\n\n        sentences[i] = ' '.join(words)\n\n    return sentences","kernel":"nlp_env","pos":23,"start":1656599466814,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599469070,"exec_count":18,"id":"89b0af","input":"stemmed_sentences = stem_sentences(sentences)\nprint(stemmed_sentences[:10])","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"['barack hussein obama ii /bəˈrɑːk huːˈseɪn oʊˈbɑːmə/ listen bə-rahk hoo-sayn oh-bah-mə ; born august is an american politician who serv as the th presid of the unit state from to', 'he wa the first african-american presid of the unit state', 'a member of the democrat parti he previous serv as a u.s. senat from illinoi from to and as an illinoi state senat from to', 'obama wa born in honolulu hawaii', 'after graduat from columbia univers in he work as a commun organ in chicago', 'in he enrol in harvard law school where he wa the first black presid of the harvard law review', 'after graduat he becam a civil right attorney and an academ teach constitut law at the univers of chicago law school from to', 'turn to elect polit he repres the th district in the illinoi senat from until when he ran for the u.s. senat', 'obama receiv nation attent in with hi march senat primari win hi well-receiv juli democrat nation convent keynot address and hi landslid novemb elect to the senat', 'in a year after begin hi campaign and after a close primari campaign against hillari clinton he wa nomin by the democrat parti for presid']\n"}},"pos":24,"start":1656599468654,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599477907,"exec_count":19,"id":"c16cba","input":"import nltk\nnltk.download('omw-1.4')","kernel":"nlp_env","output":{"0":{"name":"stderr","text":"[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n"},"1":{"data":{"text/plain":"True"},"exec_count":19}},"pos":26,"start":1656599477745,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599481158,"exec_count":20,"id":"3f6de2","input":"from nltk.stem import WordNetLemmatizer\n\n## Step 1: Import the lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n'''\nYour code here: Define a function called \"lem_sentences\" that: loops through the sentences, split the sentences up by words and applies \"lemmatizer.lemmatize\" to each word and then join everything back into a sentence\n'''\n##Similar to stopwords: For loop through the sentences, split by words and apply \"lemmatizer.lemmatize\" to each word and join back into a sentence\ndef lem_sentences(sentences):\n\n    for i in range(len(sentences)):\n\n        words = []\n\n        word_list = nltk.word_tokenize(sentences[i])\n\n        for word in word_list:\n            words.append(lemmatizer.lemmatize(word))\n\n        sentences[i] = ' '.join(words)\n\n    return sentences","kernel":"nlp_env","pos":27,"start":1656599481151,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599487437,"exec_count":21,"id":"068589","input":"sentences = lem_sentences(sentences)\nprint(sentences[:10])","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"['barack hussein obama ii /bəˈrɑːk huːˈseɪn oʊˈbɑːmə/ listen bə-rahk hoo-sayn oh-bah-mə ; born august is an american politician who serv a the th presid of the unit state from to', 'he wa the first african-american presid of the unit state', 'a member of the democrat parti he previous serv a a u.s. senat from illinoi from to and a an illinoi state senat from to', 'obama wa born in honolulu hawaii', 'after graduat from columbia univers in he work a a commun organ in chicago', 'in he enrol in harvard law school where he wa the first black presid of the harvard law review', 'after graduat he becam a civil right attorney and an academ teach constitut law at the univers of chicago law school from to', 'turn to elect polit he repres the th district in the illinoi senat from until when he ran for the u.s. senat', 'obama receiv nation attent in with hi march senat primari win hi well-receiv juli democrat nation convent keynot address and hi landslid novemb elect to the senat', 'in a year after begin hi campaign and after a close primari campaign against hillari clinton he wa nomin by the democrat parti for presid']\n"}},"pos":28,"start":1656599485599,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599491252,"exec_count":22,"id":"2b313d","input":"nltk.download('averaged_perceptron_tagger')","kernel":"nlp_env","output":{"0":{"name":"stderr","text":"[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"},"1":{"data":{"text/plain":"True"},"exec_count":22}},"pos":30,"start":1656599491232,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599494551,"exec_count":23,"id":"f6c50a","input":"# POS Tagging example\n# CC - coordinating conjunction\n# NN - noun, singular (cat, tree)\nall_words = nltk.word_tokenize(text)  ###If we want to look at part of speech taking before we stem/lem\n\ntagged_words = nltk.pos_tag(all_words)\n##Creates a list of lists where each element of the list is [word,partofspeech abbreviation]\n\n# Tagged word paragraph\nword_tags = []\nfor tw in tagged_words:\n    word_tags.append(tw[0]+\"_\"+tw[1])\n\ntagged_paragraph = ' '.join(word_tags)\n\n'''\nYour code here: print the first 1000 characters of tagged_paragraph.\n'''\ntagged_paragraph[:1000]","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"'barack_NN hussein_NN obama_NN ii_NN (_( /bəˈrɑːk_JJ huːˈseɪn_NN oʊˈbɑːmə/_NN (_( listen_JJ )_) bə-rahk_JJ hoo-sayn_JJ oh-bah-mə_NN ;_: born_VBN august_RB ,_, )_) is_VBZ an_DT american_JJ politician_NN who_WP served_VBD as_IN the_DT th_NN president_NN of_IN the_DT united_JJ states_NNS from_IN to_TO ._. he_PRP was_VBD the_DT first_JJ african-american_JJ president_NN of_IN the_DT united_JJ states_NNS ._. a_DT member_NN of_IN the_DT democratic_JJ party_NN ,_, he_PRP previously_RB served_VBD as_IN a_DT u.s._JJ senator_NN from_IN illinois_NN from_IN to_TO and_CC as_IN an_DT illinois_NN state_NN senator_NN from_IN to_TO ._. obama_NN was_VBD born_VBN in_IN honolulu_NN ,_, hawaii_NN ._. after_IN graduating_VBG from_IN columbia_NN university_NN in_IN ,_, he_PRP worked_VBD as_IN a_DT community_NN organizer_NN in_IN chicago_NN ._. in_IN ,_, he_PRP enrolled_VBD in_IN harvard_NN law_NN school_NN ,_, where_WRB he_PRP was_VBD the_DT first_JJ black_JJ president_NN of_IN the_DT harvard_NN law_NN review_'"},"exec_count":23}},"pos":31,"start":1656599493955,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599498495,"exec_count":24,"id":"8daaef","input":"# Install gensim - pip install gensim\nimport nltk\nfrom gensim.models import Word2Vec\nimport matplotlib.pyplot as plt\nnltk.download('punkt')\nfrom wordcloud import WordCloud","kernel":"nlp_env","output":{"0":{"name":"stderr","text":"[nltk_data] Downloading package punkt to /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"}},"pos":33,"start":1656599497505,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599500740,"exec_count":25,"id":"9de68d","input":"#Let's go ahead and create a list that's formatted how word2vec needs:\n    # a list of lists where the ith entry in the list is the word tokenizaiton of the ith sentence (after preprocessing)\n\ntokenized = []\n\nfor sentence in sentences:\n    tokenized.append(nltk.word_tokenize(sentence))","kernel":"nlp_env","pos":34,"start":1656599500679,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599502806,"exec_count":26,"id":"230d54","input":"# print the tokenized list of lists\nprint(tokenized[:10])","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"[['barack', 'hussein', 'obama', 'ii', '/bəˈrɑːk', 'huːˈseɪn', 'oʊˈbɑːmə/', 'listen', 'bə-rahk', 'hoo-sayn', 'oh-bah-mə', ';', 'born', 'august', 'is', 'an', 'american', 'politician', 'who', 'serv', 'a', 'the', 'th', 'presid', 'of', 'the', 'unit', 'state', 'from', 'to'], ['he', 'wa', 'the', 'first', 'african-american', 'presid', 'of', 'the', 'unit', 'state'], ['a', 'member', 'of', 'the', 'democrat', 'parti', 'he', 'previous', 'serv', 'a', 'a', 'u.s.', 'senat', 'from', 'illinoi', 'from', 'to', 'and', 'a', 'an', 'illinoi', 'state', 'senat', 'from', 'to'], ['obama', 'wa', 'born', 'in', 'honolulu', 'hawaii'], ['after', 'graduat', 'from', 'columbia', 'univers', 'in', 'he', 'work', 'a', 'a', 'commun', 'organ', 'in', 'chicago'], ['in', 'he', 'enrol', 'in', 'harvard', 'law', 'school', 'where', 'he', 'wa', 'the', 'first', 'black', 'presid', 'of', 'the', 'harvard', 'law', 'review'], ['after', 'graduat', 'he', 'becam', 'a', 'civil', 'right', 'attorney', 'and', 'an', 'academ', 'teach', 'constitut', 'law', 'at', 'the', 'univers', 'of', 'chicago', 'law', 'school', 'from', 'to'], ['turn', 'to', 'elect', 'polit', 'he', 'repres', 'the', 'th', 'district', 'in', 'the', 'illinoi', 'senat', 'from', 'until', 'when', 'he', 'ran', 'for', 'the', 'u.s.', 'senat'], ['obama', 'receiv', 'nation', 'attent', 'in', 'with', 'hi', 'march', 'senat', 'primari', 'win', 'hi', 'well-receiv', 'juli', 'democrat', 'nation', 'convent', 'keynot', 'address', 'and', 'hi', 'landslid', 'novemb', 'elect', 'to', 'the', 'senat'], ['in', 'a', 'year', 'after', 'begin', 'hi', 'campaign', 'and', 'after', 'a', 'close', 'primari', 'campaign', 'against', 'hillari', 'clinton', 'he', 'wa', 'nomin', 'by', 'the', 'democrat', 'parti', 'for', 'presid']]\n"}},"pos":35,"start":1656599502800,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599510959,"exec_count":28,"id":"c91f84","input":"#print the first 10 most common words.\nmost_common_words[:10]","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"['the', 'in', 'a', 'of', 'and', 'to', 'obama', '``', 'on', 'he']"},"exec_count":28}},"pos":38,"start":1656599510955,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599634603,"exec_count":33,"id":"7876f2","input":"''' Training the Word2Vec model. You should pass:\n1. a list of lists where the ith entry in the list is the word tokenizaiton of the ith sentence\n2. min_count=1 --> Ignores all words with total frequency lower than 1 (i.e., include everything).\n'''\n# create the model\nmodel = Word2Vec(tokenized, min_count=1)\n\n# get the most common words of the model (it's entire vocabulary)\nmost_common_words = model.wv.index_to_key\n\n# save the model to use it later\nmodel.save(\"word2vec.model\")\n\n# model = Word2Vec.load(\"word2vec.model\")","kernel":"nlp_env","pos":37,"start":1656599634479,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599638788,"exec_count":34,"id":"047aa9","input":"model.wv.most_similar('obama')","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"[('the', 0.9996244311332703),\n ('a', 0.9996238350868225),\n ('and', 0.9995702505111694),\n ('of', 0.9995452165603638),\n ('in', 0.9995089769363403),\n ('to', 0.9994973540306091),\n ('with', 0.9993702173233032),\n ('for', 0.9993687868118286),\n ('``', 0.9993574619293213),\n (\"'s\", 0.99932861328125)]"},"exec_count":34}},"pos":40,"start":1656599638780,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599644304,"exec_count":35,"id":"4d6015","input":"    # Finding Word Vectors - print word vectors for certain words in your text\nvector = model.wv['global']\nprint(vector)","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"[-3.65482620e-03  2.22611632e-02  8.40011798e-03 -8.95327143e-03\n -1.18108634e-02 -3.01626921e-02  9.65650566e-03  4.88639213e-02\n -1.60328690e-02 -8.92803725e-03 -9.79553536e-03 -4.14335839e-02\n -1.21677285e-02  3.46108660e-04  5.29623497e-03 -1.72571186e-02\n  2.10888144e-02 -1.63829345e-02 -1.03364959e-02 -4.43437211e-02\n  2.05388051e-02  1.06121451e-02  3.31876948e-02 -6.66833157e-03\n -1.99390436e-03 -4.11374681e-03 -2.37888191e-02 -7.15416158e-03\n -2.74949931e-02  1.10504376e-02  3.72038856e-02 -8.18759203e-03\n -1.83547498e-03 -1.82031821e-02 -4.68579028e-03  2.72821300e-02\n  8.02880153e-03 -2.94182263e-02 -9.23311338e-03 -1.64785907e-02\n  9.11288988e-03 -2.05078311e-02 -9.00646485e-03  1.26754837e-02\n  5.03888819e-03 -6.56894222e-03 -1.13775404e-02 -1.31037002e-02\n  1.52961807e-02  1.63434520e-02  1.91070698e-02 -9.64963716e-03\n  1.64147140e-03  4.61350055e-03 -1.55157726e-02 -7.48492021e-05\n  2.71351226e-02  3.83876357e-03 -8.25414434e-03 -7.75126973e-03\n -5.51005360e-03 -1.80794706e-03  9.81453713e-03 -9.05263331e-03\n -2.76373960e-02  2.26116385e-02 -3.24143562e-03  3.95036396e-03\n -2.83201840e-02  1.83965899e-02 -1.67149771e-02  2.19674371e-02\n  1.92534346e-02 -2.05358537e-03  1.93155073e-02  1.03428792e-02\n  5.31841535e-03  1.34262228e-02 -1.11606354e-02  1.71675384e-02\n -2.33578477e-02  3.94078251e-03 -1.78689230e-02  2.58521251e-02\n -5.13107330e-03 -5.76900272e-03  5.37860207e-03  1.85415968e-02\n  1.11789154e-02  5.16200298e-03  1.66463293e-02  1.40995570e-02\n -1.50108477e-03 -1.58158748e-03  4.90171127e-02  2.82463431e-02\n  2.63877343e-02 -1.16471583e-02 -4.59980406e-03 -3.55641847e-03]\n"}},"pos":42,"start":1656599644293,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599734977,"exec_count":37,"id":"b0d0c7","input":"# code to print a wordcloud for your sentences\n\n!pip install wordcloud\nwordcloud = WordCloud(\n                        background_color='white',\n                        max_words=100,\n                        max_font_size=50, \n                        random_state=42\n                        ).generate(str(sentences))\nfig = plt.figure(1)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"Requirement already satisfied: wordcloud in /projects/6a848d56-5da1-43fb-bfc9-9172977ceeb4/.local/lib/python3.8/site-packages (1.8.2.2)\r\n"},"1":{"name":"stdout","text":"Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.8/dist-packages (from wordcloud) (1.22.3)\r\nRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from wordcloud) (9.1.0)\r\nRequirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from wordcloud) (3.1.2)\r\n"},"2":{"data":{"text/plain":"<Figure size 432x288 with 0 Axes>"}},"3":{"data":{"image/png":"458e880724fbc9c62d64418022388157e5215393","text/plain":"<Figure size 720x720 with 1 Axes>"},"metadata":{"needs_background":"light"}}},"pos":43,"start":1656599732534,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599889729,"exec_count":38,"id":"0a4051","input":"# reFetching the data\nlame_text = \"\"\nfor paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n    lame_text += paragraph.text","kernel":"nlp_env","pos":45,"start":1656599889687,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599937518,"exec_count":40,"id":"7c7da9","input":"# Redo the word cloud but set stopwords to empty so it looks really bad\nwordcloud = WordCloud(\n                        background_color='white',\n                        max_words=100,\n                        max_font_size=50, \n                        random_state=42, ###SET STOPWORDS = [] and/or include_numbers = True or you will get the same thing!!!\n                        stopwords = [''],\n                        include_numbers = True).generate(str(lame_text)) \nfig = plt.figure(1)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"<Figure size 432x288 with 0 Axes>"}},"1":{"data":{"image/png":"decb8f1dc53fe7f989929c81fd254fde80ce3f85","text/plain":"<Figure size 720x720 with 1 Axes>"},"metadata":{"needs_background":"light"}}},"pos":47,"start":1656599936880,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599976556,"exec_count":41,"id":"bbf3a7","input":"# Training the Word2Vec model (same code as before), but one change: use our lame data that was not preprocessed\n\n# Try printing this after training the model.\nwords = model.wv.index_to_key\nprint(words[:10])","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"['the', 'in', 'a', 'of', 'and', 'to', 'obama', '``', 'on', 'he']\n"}},"pos":48,"start":1656599976548,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"17306b","input":"# Finding a vector of a word, but badly","pos":49,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"578107","input":"","pos":51,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"a0766e","input":"'''\nDoing the same without removing stop words or lemming\n'''\n# tokenize the text using sent_tokenize\n\n# from this list of sentences, create a list of lists where the ith entry in the list is the word tokenizaiton of the ith sentence (after preprocessing)","pos":46,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c6255f","input":"### Finding the most similar words in the model but... you get the idea ###\n\n","pos":50,"type":"cell"}
{"cell_type":"code","exec_count":67,"id":"3efd1c","input":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords","output":{"0":{"name":"stderr","output_type":"stream","text":"[nltk_data] Downloading package stopwords to\n[nltk_data]     /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"}},"pos":18,"type":"cell"}
{"cell_type":"code","exec_count":80,"id":"a959e5","input":"# Look up the most similar words to certain words in your text using the model.wv.most_similar() function","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"1e66dc","input":"## NLP Part 0 - Get some Data!\n\nThis section's code is mostly given to you as a review for how you can scrape and manipulate data from the web. \n\n","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"27668d","input":"## Training the Word2Vec model\n\nFor this part you may want to follow a guide [here](https://radimrehurek.com/gensim/models/word2vec.html). ","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"3390bf","input":"# Word2Vec Model Visualization","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"48aaf2","input":"A regex is a string of text that allows you to create patterns that help match, locate, and manage text.\n\n**Good resources:** \n\nregular expression tester: https://regex101.com/\n\n","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"55f23c","input":"## NLP Part 2 - Stopwords and Punctuation\nNow we are going to work to remove stopwords and punctuation from our data. Why do you think we are going to do this? Do some research if you don't know yet. ","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"680fd9","input":"#### Basic Preprocessing Using Regular Expression \\(Regex\\)\n\n","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"6f85cf","input":"## NLP Part 1 - Tokenization of paragraphs/sentences\n\nIn this section we are going to tokenize our sentences and words. If you aren't familiar with tokenization, we recommend looking up \"what is tokenization\". \n\nYou should also spend time on the [NLTK documentation](https://www.nltk.org/). If you're not sure how to do something, or get an error, it is best to google it first and ask questions as you go!\n\n","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"81e670","input":"## NLP Part 3b - Lemmatization\nLemmatization considers the context and converts the word to its meaningful base form. There is a cool tutorial and definition of lemmatization in NLTK [here](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/).","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"82dba0","input":"## Reflection\nHow important do you think proper preprocessing in NLP is?","pos":52,"type":"cell"}
{"cell_type":"markdown","id":"990460","input":"","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"aa13eb","input":"## Testing our model","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"b81272","input":"### Why did we do all this work?","pos":44,"type":"cell"}
{"cell_type":"markdown","id":"c920fe","input":"## NLP Part 4 - POS Tagging\nParts of speech tagging is marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context.","pos":29,"type":"cell"}
{"cell_type":"markdown","id":"d03e4b","input":"## NLP Part 3a - Stemming the words\nStemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. There is an example below!","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"d94751","input":"# Natural Language Processing using NLTK","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"f9368c","input":"","pos":53,"type":"cell"}
{"id":"361c64","input":"","pos":4.5,"type":"cell"}
{"id":"6d7fcd","input":"","pos":4.25,"type":"cell"}
{"id":0,"time":1656600294808,"type":"user"}
{"last_load":1656599387033,"type":"file"}