{"backend_state":"running","connection_file":"/projects/6a848d56-5da1-43fb-bfc9-9172977ceeb4/.local/share/jupyter/runtime/kernel-7ddc049c-1319-4a49-93dd-2d506ea3f582.json","kernel":"nlp_env","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"interpreter":{"hash":"335ee12212264728feb72f243af72c5a8ea26c832f07e1f651ce9e17c7ceae23"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1656523372133,"exec_count":32,"id":"f0ed06","input":"'''\nYour code here: Tokenize the words from the data and set it to a variable called words.\nHint: how to do this might be on the very home page of NLTK!\n'''\nwords = nltk.word_tokenize(text)","kernel":"nlp_env","pos":8,"start":1656523372061,"state":"done","type":"cell"}
{"cell_type":"code","end":1656523379958,"exec_count":34,"id":"ac4cf4","input":"'''\nYour code here: Tokenize the sentences from the data  and set it to a variable called sentences.\nHint: try googling how to tokenize sentences in NLTK!\n'''\nsentences = nltk.sent_tokenize(text)","kernel":"nlp_env","pos":10,"start":1656523379939,"state":"done","type":"cell"}
{"cell_type":"code","end":1656523383263,"exec_count":35,"id":"af1d02","input":"print(sentences[:10])","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"[\" contemporary climate change includes both global warming and its impacts on earth's weather patterns.\", 'there have been previous periods of climate change, but the current changes are distinctly more rapid and not due to natural causes.', 'instead, they are caused by the emission of greenhouse gases, mostly carbon dioxide (co ) and methane.', 'burning fossil fuels for energy use creates most of these emissions.', 'certain agricultural practices, industrial processes, and forest loss are additional sources.', \"greenhouse gases are transparent to sunlight, allowing it through to heat the earth's surface.\", \"when the earth emits that heat as infrared radiation the gases absorb it, trapping the heat near the earth's surface.\", 'as the planet heats up it causes changes like the loss of sunlight-reflecting snow cover, amplifying global warming.', 'due to climate change, deserts are expanding, while heat waves and wildfires are becoming more common.', 'increased warming in the arctic has contributed to melting permafrost, glacial retreat and sea ice loss.']\n"}},"pos":11,"start":1656523383243,"state":"done","type":"cell"}
{"cell_type":"code","end":1656523410625,"exec_count":36,"id":"ef3851","input":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords","kernel":"nlp_env","output":{"0":{"name":"stderr","text":"[nltk_data] Downloading package stopwords to\n[nltk_data]     /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"}},"pos":13,"start":1656523410548,"state":"done","type":"cell"}
{"cell_type":"code","end":1656523416277,"exec_count":37,"id":"6f3b45","input":"'''\ndefine a function called \"remove_stopwords\" that takes in a list of the sentences of the text and returns one that doesn't have any stopwords.\n'''\ndef remove_stopwords(sentences):\n    \n    ### Some code goes here. Hint: You may have to look up how to remove stopwords in NLTK if you get stuck. ###\n    for i in range(len(sentences)):\n        # i = position of each sentence; i is a number\n        words = []\n        \n        word_list = nltk.word_tokenize(sentences[i])\n        \n        for word in word_list:\n            if word not in stopwords.words ('english'):\n                words.append(word)\n        \n        sentences[i] = ' '.join(words)\n        \n    return sentences\n\n###Then actually apply your function###\nsentences = remove_stopwords(sentences)\nprint(sentences[:10]) #Check if it worked correctly. Are all stopwords removed?","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"[\"contemporary climate change includes global warming impacts earth 's weather patterns .\", 'previous periods climate change , current changes distinctly rapid due natural causes .', 'instead , caused emission greenhouse gases , mostly carbon dioxide ( co ) methane .', 'burning fossil fuels energy use creates emissions .', 'certain agricultural practices , industrial processes , forest loss additional sources .', \"greenhouse gases transparent sunlight , allowing heat earth 's surface .\", \"earth emits heat infrared radiation gases absorb , trapping heat near earth 's surface .\", 'planet heats causes changes like loss sunlight-reflecting snow cover , amplifying global warming .', 'due climate change , deserts expanding , heat waves wildfires becoming common .', 'increased warming arctic contributed melting permafrost , glacial retreat sea ice loss .']\n"}},"pos":14,"scrolled":true,"start":1656523415197,"state":"done","type":"cell"}
{"cell_type":"code","end":1656523418369,"exec_count":38,"id":"5f2f7f","input":"'''\ndefine a function called \"remove_punctuation\" that removes punctuation from the sentences.\n'''\ndef remove_punctuation(sentences):\n    \n    ### Some code goes here. Hint: Try looking up how to remove punctuation in NLTK if you get stuck. ###\n    for i in range(len(sentences)):\n        # i = position of each sentence; i is a number\n        words = []\n        \n        word_list = nltk.word_tokenize(sentences[i])\n        \n        for word in word_list:\n            if word not in \",.?!()\":\n                words.append(word)\n        \n        sentences[i] = ' '.join(words)\n        \n    return sentences\n        \n    return sentences\nsentences = remove_punctuation(sentences)\nprint(sentences[:10]) #eliminating all punctuation.","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"[\"contemporary climate change includes global warming impacts earth 's weather patterns\", 'previous periods climate change current changes distinctly rapid due natural causes', 'instead caused emission greenhouse gases mostly carbon dioxide co methane', 'burning fossil fuels energy use creates emissions', 'certain agricultural practices industrial processes forest loss additional sources', \"greenhouse gases transparent sunlight allowing heat earth 's surface\", \"earth emits heat infrared radiation gases absorb trapping heat near earth 's surface\", 'planet heats causes changes like loss sunlight-reflecting snow cover amplifying global warming', 'due climate change deserts expanding heat waves wildfires becoming common', 'increased warming arctic contributed melting permafrost glacial retreat sea ice loss']\n"}},"pos":15,"start":1656523418311,"state":"done","type":"cell"}
{"cell_type":"code","end":1656523422309,"exec_count":39,"id":"5fc0ac","input":"from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n# try each of the words below\nstemmer.stem('troubled')\nstemmer.stem('trouble')\nstemmer.stem('troubling')\nstemmer.stem('troubles')","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"'troubl'"},"exec_count":39}},"pos":17,"start":1656523422293,"state":"done","type":"cell"}
{"cell_type":"code","end":1656523993869,"exec_count":44,"id":"c21d81","input":"from nltk.stem import WordNetLemmatizer\n    \n## Step 1: Import the lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n'''\nYour code here: Define a function called \"lem_sentences\" that: loops through the sentences, split the sentences up by words and applies \"lemmatizer.lemmatize\" to each word and then join everything back into a sentence\n'''\n##Similar to stopwords: For loop through the sentences, split by words and apply \"lemmatizer.lemmatize\" to each word and join back into a sentence\ndef lem_sentences(sentences):\n    for i in range(len(sentences)):\n        # i = position of each sentence; i is a number\n        words = []\n        \n        word_list = nltk.word_tokenize(sentences[i])\n        \n        for word in word_list:\n            words.append(lemmatizer.lemmatize(word))\n        \n        sentences[i] = ' '.join(words)\n        \n    return sentences\nsentences = lem_sentences(sentences)\nprint(sentences[:10]) ","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"[\"contemporari contemporari contemporary contemporary climat climat climate climate chang chang change change includ includ includes includes global global global global warm warm warming warming impact impact impact impact earth earth earth earth 's 's 's 's weather weather weather weather pattern pattern pattern pattern\", 'previou previou previous previous period period period period climat climat climate climate chang chang change change current current current current chang chang change change distinctli distinctli distinctly distinctly rapid rapid rapid rapid due due due due natur natur natural natural caus caus cause cause', 'instead instead instead instead caus caus caused caused emiss emiss emission emission greenhous greenhous greenhouse greenhouse gase gase gas gas mostli mostli mostly mostly carbon carbon carbon carbon dioxid dioxid dioxide dioxide co co co co methan methan methane methane', 'burn burn burning burning fossil fossil fossil fossil fuel fuel fuel fuel energi energi energy energy use use use use creat creat creates creates emiss emiss emission emission', 'certain certain certain certain agricultur agricultur agricultural agricultural practic practic practice practice industri industri industrial industrial process process process process forest forest forest forest loss loss loss loss addit addit additional additional sourc sourc source source', \"greenhous greenhous greenhouse greenhouse gase gase gas gas transpar transpar transparent transparent sunlight sunlight sunlight sunlight allow allow allowing allowing heat heat heat heat earth earth earth earth 's 's 's 's surfac surfac surface surface\", \"earth earth earth earth emit emit emits emits heat heat heat heat infrar infrar infrared infrared radiat radiat radiation radiation gase gase gas gas absorb absorb absorb absorb trap trap trapping trapping heat heat heat heat near near near near earth earth earth earth 's 's 's 's surfac surfac surface surface\", 'planet planet planet planet heat heat heat heat caus caus cause cause chang chang change change like like like like loss loss loss loss sunlight-reflect sunlight-reflect sunlight-reflecting sunlight-reflecting snow snow snow snow cover cover cover cover amplifi amplifi amplifying amplifying global global global global warm warm warming warming', 'due due due due climat climat climate climate chang chang change change desert desert desert desert expand expand expanding expanding heat heat heat heat wave wave wave wave wildfir wildfir wildfire wildfire becom becom becoming becoming common common common common', 'increas increas increased increased warm warm warming warming arctic arctic arctic arctic contribut contribut contributed contributed melt melt melting melting permafrost permafrost permafrost permafrost glacial glacial glacial glacial retreat retreat retreat retreat sea sea sea sea ice ice ice ice loss loss loss loss']\n"}},"pos":21,"start":1656523993311,"state":"done","type":"cell"}
{"cell_type":"code","end":1656524001551,"exec_count":45,"id":"a2ead1","input":"print(sentences[:10]) ","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"[\"contemporari contemporari contemporary contemporary climat climat climate climate chang chang change change includ includ includes includes global global global global warm warm warming warming impact impact impact impact earth earth earth earth 's 's 's 's weather weather weather weather pattern pattern pattern pattern\", 'previou previou previous previous period period period period climat climat climate climate chang chang change change current current current current chang chang change change distinctli distinctli distinctly distinctly rapid rapid rapid rapid due due due due natur natur natural natural caus caus cause cause', 'instead instead instead instead caus caus caused caused emiss emiss emission emission greenhous greenhous greenhouse greenhouse gase gase gas gas mostli mostli mostly mostly carbon carbon carbon carbon dioxid dioxid dioxide dioxide co co co co methan methan methane methane', 'burn burn burning burning fossil fossil fossil fossil fuel fuel fuel fuel energi energi energy energy use use use use creat creat creates creates emiss emiss emission emission', 'certain certain certain certain agricultur agricultur agricultural agricultural practic practic practice practice industri industri industrial industrial process process process process forest forest forest forest loss loss loss loss addit addit additional additional sourc sourc source source', \"greenhous greenhous greenhouse greenhouse gase gase gas gas transpar transpar transparent transparent sunlight sunlight sunlight sunlight allow allow allowing allowing heat heat heat heat earth earth earth earth 's 's 's 's surfac surfac surface surface\", \"earth earth earth earth emit emit emits emits heat heat heat heat infrar infrar infrared infrared radiat radiat radiation radiation gase gase gas gas absorb absorb absorb absorb trap trap trapping trapping heat heat heat heat near near near near earth earth earth earth 's 's 's 's surfac surfac surface surface\", 'planet planet planet planet heat heat heat heat caus caus cause cause chang chang change change like like like like loss loss loss loss sunlight-reflect sunlight-reflect sunlight-reflecting sunlight-reflecting snow snow snow snow cover cover cover cover amplifi amplifi amplifying amplifying global global global global warm warm warming warming', 'due due due due climat climat climate climate chang chang change change desert desert desert desert expand expand expanding expanding heat heat heat heat wave wave wave wave wildfir wildfir wildfire wildfire becom becom becoming becoming common common common common', 'increas increas increased increased warm warm warming warming arctic arctic arctic arctic contribut contribut contributed contributed melt melt melting melting permafrost permafrost permafrost permafrost glacial glacial glacial glacial retreat retreat retreat retreat sea sea sea sea ice ice ice ice loss loss loss loss']\n"}},"pos":22,"start":1656524001539,"state":"done","type":"cell"}
{"cell_type":"code","end":1656524055463,"exec_count":46,"id":"b66bd3","input":"nltk.download('averaged_perceptron_tagger')","kernel":"nlp_env","output":{"0":{"name":"stderr","text":"[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"},"1":{"data":{"text/plain":"True"},"exec_count":46}},"pos":24,"start":1656524055451,"state":"done","type":"cell"}
{"cell_type":"code","end":1656524132830,"exec_count":48,"id":"b5d731","input":"# POS Tagging example\n# CC - coordinating conjunction\n# NN - noun, singular (cat, tree)\nall_words = nltk.word_tokenize(text)  ###If we want to look at part of speech taking before we stem/lem\n\ntagged_words = nltk.pos_tag(all_words)\n##Creates a list of lists where each element of the list is [word,partofspeech abbreviation]\n\n# Tagged word paragraph\nword_tags = []\nfor tw in tagged_words:\n    word_tags.append(tw[0]+\"_\"+tw[1])\n\ntagged_paragraph = ' '.join(word_tags)\n\n'''\nYour code here: print the first 1000 characters of tagged_paragraph.\n'''\n\ntagged_paragraph[:1000]","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"\"contemporary_JJ climate_NN change_NN includes_VBZ both_DT global_JJ warming_NN and_CC its_PRP$ impacts_NNS on_IN earth_NN 's_POS weather_NN patterns_NNS ._. there_EX have_VBP been_VBN previous_JJ periods_NNS of_IN climate_NN change_NN ,_, but_CC the_DT current_JJ changes_NNS are_VBP distinctly_RB more_RBR rapid_JJ and_CC not_RB due_JJ to_TO natural_JJ causes_NNS ._. instead_RB ,_, they_PRP are_VBP caused_VBN by_IN the_DT emission_NN of_IN greenhouse_NN gases_NNS ,_, mostly_RB carbon_NN dioxide_NN (_( co_NN )_) and_CC methane_NN ._. burning_VBG fossil_JJ fuels_NNS for_IN energy_NN use_NN creates_VBZ most_JJS of_IN these_DT emissions_NNS ._. certain_JJ agricultural_JJ practices_NNS ,_, industrial_JJ processes_NNS ,_, and_CC forest_JJS loss_NN are_VBP additional_JJ sources_NNS ._. greenhouse_NN gases_NNS are_VBP transparent_JJ to_TO sunlight_VB ,_, allowing_VBG it_PRP through_IN to_TO heat_VB the_DT earth_NN 's_POS surface_NN ._. when_WRB the_DT earth_NN emits_VBZ that_DT heat_NN as_IN in\""},"exec_count":48}},"pos":25,"start":1656524132019,"state":"done","type":"cell"}
{"cell_type":"code","end":1656524245979,"exec_count":49,"id":"4f29f7","input":"# Install gensim - pip install gensim\nimport nltk\nfrom gensim.models import Word2Vec\nimport matplotlib.pyplot as plt\nnltk.download('punkt')\nfrom wordcloud import WordCloud","kernel":"nlp_env","output":{"0":{"name":"stderr","text":"[nltk_data] Downloading package punkt to /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"}},"pos":27,"start":1656524244946,"state":"done","type":"cell"}
{"cell_type":"code","end":1656524651051,"exec_count":53,"id":"16cfa8","input":"# print the tokenized list of lists\ntokenized[:10]","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"[['contemporari',\n  'contemporari',\n  'contemporary',\n  'contemporary',\n  'climat',\n  'climat',\n  'climate',\n  'climate',\n  'chang',\n  'chang',\n  'change',\n  'change',\n  'includ',\n  'includ',\n  'includes',\n  'includes',\n  'global',\n  'global',\n  'global',\n  'global',\n  'warm',\n  'warm',\n  'warming',\n  'warming',\n  'impact',\n  'impact',\n  'impact',\n  'impact',\n  'earth',\n  'earth',\n  'earth',\n  'earth',\n  \"'s\",\n  \"'s\",\n  \"'s\",\n  \"'s\",\n  'weather',\n  'weather',\n  'weather',\n  'weather',\n  'pattern',\n  'pattern',\n  'pattern',\n  'pattern'],\n ['previou',\n  'previou',\n  'previous',\n  'previous',\n  'period',\n  'period',\n  'period',\n  'period',\n  'climat',\n  'climat',\n  'climate',\n  'climate',\n  'chang',\n  'chang',\n  'change',\n  'change',\n  'current',\n  'current',\n  'current',\n  'current',\n  'chang',\n  'chang',\n  'change',\n  'change',\n  'distinctli',\n  'distinctli',\n  'distinctly',\n  'distinctly',\n  'rapid',\n  'rapid',\n  'rapid',\n  'rapid',\n  'due',\n  'due',\n  'due',\n  'due',\n  'natur',\n  'natur',\n  'natural',\n  'natural',\n  'caus',\n  'caus',\n  'cause',\n  'cause'],\n ['instead',\n  'instead',\n  'instead',\n  'instead',\n  'caus',\n  'caus',\n  'caused',\n  'caused',\n  'emiss',\n  'emiss',\n  'emission',\n  'emission',\n  'greenhous',\n  'greenhous',\n  'greenhouse',\n  'greenhouse',\n  'gase',\n  'gase',\n  'gas',\n  'gas',\n  'mostli',\n  'mostli',\n  'mostly',\n  'mostly',\n  'carbon',\n  'carbon',\n  'carbon',\n  'carbon',\n  'dioxid',\n  'dioxid',\n  'dioxide',\n  'dioxide',\n  'co',\n  'co',\n  'co',\n  'co',\n  'methan',\n  'methan',\n  'methane',\n  'methane'],\n ['burn',\n  'burn',\n  'burning',\n  'burning',\n  'fossil',\n  'fossil',\n  'fossil',\n  'fossil',\n  'fuel',\n  'fuel',\n  'fuel',\n  'fuel',\n  'energi',\n  'energi',\n  'energy',\n  'energy',\n  'use',\n  'use',\n  'use',\n  'use',\n  'creat',\n  'creat',\n  'creates',\n  'creates',\n  'emiss',\n  'emiss',\n  'emission',\n  'emission'],\n ['certain',\n  'certain',\n  'certain',\n  'certain',\n  'agricultur',\n  'agricultur',\n  'agricultural',\n  'agricultural',\n  'practic',\n  'practic',\n  'practice',\n  'practice',\n  'industri',\n  'industri',\n  'industrial',\n  'industrial',\n  'process',\n  'process',\n  'process',\n  'process',\n  'forest',\n  'forest',\n  'forest',\n  'forest',\n  'loss',\n  'loss',\n  'loss',\n  'loss',\n  'addit',\n  'addit',\n  'additional',\n  'additional',\n  'sourc',\n  'sourc',\n  'source',\n  'source'],\n ['greenhous',\n  'greenhous',\n  'greenhouse',\n  'greenhouse',\n  'gase',\n  'gase',\n  'gas',\n  'gas',\n  'transpar',\n  'transpar',\n  'transparent',\n  'transparent',\n  'sunlight',\n  'sunlight',\n  'sunlight',\n  'sunlight',\n  'allow',\n  'allow',\n  'allowing',\n  'allowing',\n  'heat',\n  'heat',\n  'heat',\n  'heat',\n  'earth',\n  'earth',\n  'earth',\n  'earth',\n  \"'s\",\n  \"'s\",\n  \"'s\",\n  \"'s\",\n  'surfac',\n  'surfac',\n  'surface',\n  'surface'],\n ['earth',\n  'earth',\n  'earth',\n  'earth',\n  'emit',\n  'emit',\n  'emits',\n  'emits',\n  'heat',\n  'heat',\n  'heat',\n  'heat',\n  'infrar',\n  'infrar',\n  'infrared',\n  'infrared',\n  'radiat',\n  'radiat',\n  'radiation',\n  'radiation',\n  'gase',\n  'gase',\n  'gas',\n  'gas',\n  'absorb',\n  'absorb',\n  'absorb',\n  'absorb',\n  'trap',\n  'trap',\n  'trapping',\n  'trapping',\n  'heat',\n  'heat',\n  'heat',\n  'heat',\n  'near',\n  'near',\n  'near',\n  'near',\n  'earth',\n  'earth',\n  'earth',\n  'earth',\n  \"'s\",\n  \"'s\",\n  \"'s\",\n  \"'s\",\n  'surfac',\n  'surfac',\n  'surface',\n  'surface'],\n ['planet',\n  'planet',\n  'planet',\n  'planet',\n  'heat',\n  'heat',\n  'heat',\n  'heat',\n  'caus',\n  'caus',\n  'cause',\n  'cause',\n  'chang',\n  'chang',\n  'change',\n  'change',\n  'like',\n  'like',\n  'like',\n  'like',\n  'loss',\n  'loss',\n  'loss',\n  'loss',\n  'sunlight-reflect',\n  'sunlight-reflect',\n  'sunlight-reflecting',\n  'sunlight-reflecting',\n  'snow',\n  'snow',\n  'snow',\n  'snow',\n  'cover',\n  'cover',\n  'cover',\n  'cover',\n  'amplifi',\n  'amplifi',\n  'amplifying',\n  'amplifying',\n  'global',\n  'global',\n  'global',\n  'global',\n  'warm',\n  'warm',\n  'warming',\n  'warming'],\n ['due',\n  'due',\n  'due',\n  'due',\n  'climat',\n  'climat',\n  'climate',\n  'climate',\n  'chang',\n  'chang',\n  'change',\n  'change',\n  'desert',\n  'desert',\n  'desert',\n  'desert',\n  'expand',\n  'expand',\n  'expanding',\n  'expanding',\n  'heat',\n  'heat',\n  'heat',\n  'heat',\n  'wave',\n  'wave',\n  'wave',\n  'wave',\n  'wildfir',\n  'wildfir',\n  'wildfire',\n  'wildfire',\n  'becom',\n  'becom',\n  'becoming',\n  'becoming',\n  'common',\n  'common',\n  'common',\n  'common'],\n ['increas',\n  'increas',\n  'increased',\n  'increased',\n  'warm',\n  'warm',\n  'warming',\n  'warming',\n  'arctic',\n  'arctic',\n  'arctic',\n  'arctic',\n  'contribut',\n  'contribut',\n  'contributed',\n  'contributed',\n  'melt',\n  'melt',\n  'melting',\n  'melting',\n  'permafrost',\n  'permafrost',\n  'permafrost',\n  'permafrost',\n  'glacial',\n  'glacial',\n  'glacial',\n  'glacial',\n  'retreat',\n  'retreat',\n  'retreat',\n  'retreat',\n  'sea',\n  'sea',\n  'sea',\n  'sea',\n  'ice',\n  'ice',\n  'ice',\n  'ice',\n  'loss',\n  'loss',\n  'loss',\n  'loss']]"},"exec_count":53}},"pos":29,"start":1656524650991,"state":"done","type":"cell"}
{"cell_type":"code","end":1656525017949,"exec_count":55,"id":"e4ea8c","input":"''' Training the Word2Vec model. You should pass:\n1. a list of lists where the ith entry in the list is the word tokenization of the ith sentence\n2. min_count=1 --> Ignores all words with total frequency lower than 1 (i.e., include everything).\n'''\n# create the model\nmodel = Word2Vec(tokenized, min_count=1)\n\n# get the most common words of the model (it's entire vocabulary)\nmost_common_words = model.wv.index_to_key\n\n# save the model to use it later\nmodel.save(\"word2vec.model\")\n\n# model = Word2Vec.load(\"word2vec.model\")","kernel":"nlp_env","pos":31,"start":1656525017528,"state":"done","type":"cell"}
{"cell_type":"code","end":1656525020152,"exec_count":56,"id":"e47c0c","input":"#print the first 10 most common words.\nmost_common_words[:10]","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"['climat',\n 'climate',\n 'chang',\n 'global',\n 'change',\n 'co',\n 'warm',\n 'heat',\n 'carbon',\n 'Â°c']"},"exec_count":56}},"pos":32,"start":1656525020147,"state":"done","type":"cell"}
{"cell_type":"code","end":1656525064862,"exec_count":57,"id":"59db87","input":"model.wv.most_similar('climate')","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"[('climat', 0.9962087273597717),\n ('change', 0.993963897228241),\n ('chang', 0.9882426857948303),\n ('model', 0.9724756479263306),\n ('past', 0.9710574746131897),\n ('environment', 0.9688279628753662),\n ('caus', 0.9647949934005737),\n ('environmental', 0.9644607901573181),\n ('affect', 0.964186429977417),\n ('denial', 0.9639515280723572)]"},"exec_count":57}},"pos":34,"start":1656525064846,"state":"done","type":"cell"}
{"cell_type":"code","end":1656525192235,"exec_count":58,"id":"a2e02e","input":"    # Finding Word Vectors - print word vectors for certain words in your text\nvector = model.wv['climate']\nprint(vector)","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"[-0.11805189  0.14220493  0.01727928  0.00856698  0.09396842 -0.20317508\n  0.08720521  0.38040766 -0.13583729 -0.07290927 -0.09196492 -0.31129682\n -0.05209626  0.06790291  0.0690322  -0.08850008  0.04784127 -0.11891805\n  0.03799606 -0.34842595  0.1045564   0.19170368  0.09035107 -0.10755579\n -0.08981141 -0.03042706 -0.15819387 -0.16040674 -0.2556521   0.0619145\n  0.28355628  0.06904842  0.03032665 -0.1484821  -0.07589728  0.12694661\n -0.08524486 -0.13600518 -0.08357601 -0.24148846  0.06695461 -0.13431352\n -0.04734235 -0.0279879   0.12767936 -0.04802968 -0.09374062 -0.01769661\n  0.17697856  0.13083674  0.02760128 -0.18652444 -0.02537344 -0.04667776\n -0.18232611  0.1763441   0.08262532  0.0154609  -0.16343577  0.07667255\n  0.04354695  0.08113492 -0.09542339  0.0015204  -0.17038934  0.21197084\n  0.12874073  0.06619795 -0.12164494  0.24171339 -0.10691591  0.09018554\n  0.25392264 -0.09269185  0.15623647  0.0357895   0.07452405 -0.06490318\n -0.12953252  0.02355538 -0.04787396 -0.03099236 -0.16875125  0.24616845\n -0.07355832  0.03576668  0.04058606  0.17075074  0.16476524 -0.00147099\n  0.24927247  0.03904257 -0.02271874 -0.0077862   0.21068184  0.16006437\n  0.1884092  -0.20362195  0.06888433 -0.00060867]\n"}},"pos":36,"start":1656525192226,"state":"done","type":"cell"}
{"cell_type":"code","end":1656525199204,"exec_count":59,"id":"123ed4","input":"# code to print a wordcloud for your sentences\nwordcloud = WordCloud(\n                        background_color='white',\n                        max_words=100,\n                        max_font_size=50, \n                        random_state=42\n                        ).generate(str(sentences))\nfig = plt.figure(1)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"<Figure size 432x288 with 0 Axes>"}},"1":{"data":{"image/png":"a4277b6cda6f73ed757b3b601f495a18918265b0","text/plain":"<Figure size 720x720 with 1 Axes>"},"metadata":{"needs_background":"light"}}},"pos":40,"start":1656525198560,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599685949,"exec_count":9,"id":"7de795","input":"print(words[:10])","kernel":"nlp_env","output":{"0":{"ename":"NameError","evalue":"name 'words' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mwords\u001b[49m[:\u001b[38;5;241m10\u001b[39m])\n","\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"]}},"pos":9,"start":1656599685937,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599813426,"exec_count":11,"id":"34eda7","input":"# Install NLTK - pip install nltk\nimport nltk\nnltk.download('wordnet')\nnltk.download('punkt')","kernel":"nlp_env","output":{"0":{"name":"stderr","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"},"1":{"data":{"text/plain":"True"},"exec_count":11}},"pos":2,"start":1656599812798,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599817144,"exec_count":12,"id":"549312","input":"import urllib\nimport bs4 as bs\nimport re","kernel":"nlp_env","pos":4,"start":1656599817136,"state":"done","type":"cell"}
{"cell_type":"code","end":1656599849911,"exec_count":15,"id":"cc6e80","input":"text[:100]","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"' the solar system[c] is the gravitationally bound system of the sun and the objects that orbit it. i'"},"exec_count":15}},"pos":6,"start":1656599849902,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":13,"id":"3cbde1","input":"# We will read the contents of the Wikipedia article \"Global_warming\" as an example, please feel free to use your own! You can use the url below:\nurl = 'https://en.wikipedia.org/wiki/Climate_change' # you can change this to use other sites as well.\n\n# We can open the page using \"urllib.request.urlopen\" then read it using \".read()\"\nsource = urllib.request.urlopen(url).read()\n\n# Beautiful Soup is a Python library for pulling data out of HTML and XML files.\n# you may need to install a parser library --> \"!pip3 install lxml\"\n# Parsing the data/creating BeautifulSoup object\n\nsoup = bs.BeautifulSoup(source,\"html.parser\") \n\n# Fetching the data\ntext = \"\"\nfor paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n    text += paragraph.text\n\n# Preprocessing the data\n\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # [0-9]* --> Matches zero or more repetitions of any digit from 0 to 9\ntext = re.sub(r'\\W^.?!',' ',text) # \\W --> Matches any character which is not a word character except (.?!)\ntext = re.sub(r'\\d',' ',text) # \\d --> Matches any decimal digit\ntext = re.sub(r'\\s+',' ',text) # \\s --> Matches any characters that are considered whitespace (Ex: [\\t\\n\\r\\f\\v].)\n\ntext = text.lower() #everything to lowercase","kernel":"nlp_env","pos":5,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":40,"id":"687ccd","input":"'''\nYour code here:\nDefine a function called \"stem_sentences\" that takes in a list of sentences and returns a list of stemmed sentences.\n'''\ndef stem_sentences(sentences):\n    ### Some code goes here. Hint: Try looking up how to stem words in NLTK if you get stuck (or simply use the example above and run stemmer in a loop!). ###\n    for i in range(len(sentences)):\n        # i = position of each sentence; i is a number\n        words = []\n        \n        word_list = nltk.word_tokenize(sentences[i])\n        \n        for word in word_list:\n            words.append(stemmer.stem(word))\n        \n        sentences[i] = ' '.join(words)\n        \n    return sentences\nsentences = stem_sentences(sentences)\nprint(sentences[:10]) #eliminating all stems.","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"[\"contemporari contemporary climat climate chang change includ includes global global warm warming impact impacts earth earth 's 's weather weather pattern patterns\", 'previou previous period periods climat climate chang change current current chang changes distinctli distinctly rapid rapid due due natur natural caus causes', 'instead instead caus caused emiss emission greenhous greenhouse gase gases mostli mostly carbon carbon dioxid dioxide co co methan methane', 'burn burning fossil fossil fuel fuels energi energy use use creat creates emiss emissions', 'certain certain agricultur agricultural practic practices industri industrial process processes forest forest loss loss addit additional sourc sources', \"greenhous greenhouse gase gases transpar transparent sunlight sunlight allow allowing heat heat earth earth 's 's surfac surface\", \"earth earth emit emits heat heat infrar infrared radiat radiation gase gases absorb absorb trap trapping heat heat near near earth earth 's 's surfac surface\", 'planet planet heat heats caus causes chang changes like like loss loss sunlight-reflect sunlight-reflecting snow snow cover cover amplifi amplifying global global warm warming', 'due due climat climate chang change desert deserts expand expanding heat heat wave waves wildfir wildfires becom becoming common common', 'increas increased warm warming arctic arctic contribut contributed melt melting permafrost permafrost glacial glacial retreat retreat sea sea ice ice loss loss']\n"}},"pos":18,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":52,"id":"f4376c","input":"#Let's go ahead and create a list that's formatted how word2vec needs:\n    # a list of lists where the ith entry in the list is the word tokenization of the ith sentence (after preprocessing)\ntokenized = []\n\nfor sentence in sentences:\n    tokenized.append(nltk.word_tokenize(sentence))","kernel":"nlp_env","pos":28,"state":"done","type":"cell"}
{"cell_type":"code","id":"03331f","input":"# Redo the word cloud but set stopwords to empty so it looks really bad\nwordcloud = WordCloud(\n                        background_color='white',\n                        max_words=100,\n                        max_font_size=50, \n                        random_state=42, ###SET STOPWORDS = [] and/or include_numbers = True or you will get the same thing!!!\n                        stopwords = [],\n                        include_numbers = True).generate(str(lame_sentences)) \nfig = plt.figure(1)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","pos":44,"state":"done","type":"cell"}
{"cell_type":"code","id":"0aac00","input":"### Finding the most similar words in the model but... you get the idea ###\n\n","pos":47,"state":"done","type":"cell"}
{"cell_type":"code","id":"0f67ef","input":"# Finding a vector of a word, but badly","pos":46,"state":"done","type":"cell"}
{"cell_type":"code","id":"1ca9a4","input":"similar1, similar2","pos":39,"state":"done","type":"cell"}
{"cell_type":"code","id":"2382e7","input":"# Look up the most similar words to certain words in your text using the model.wv.most_similar() function\n","pos":33,"state":"done","type":"cell"}
{"cell_type":"code","id":"2b464d","input":"# reFetching the data\nlame_text = \"\"\nfor paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n    lame_text += paragraph.text","pos":42,"state":"done","type":"cell"}
{"cell_type":"code","id":"3924e2","input":"# Training the Word2Vec model (same code as before), but one change: use our lame data that was not preprocessed\n\n# Try printing this after training the model.\nwords = model.wv.index_to_key\nprint(words[:10])","pos":45,"state":"done","type":"cell"}
{"cell_type":"code","id":"c292c4","input":"'''\nDoing the same without removing stop words or lemming\n'''\n# tokenize the text using sent_tokenize\n\n# from this list of sentences, create a list of lists where the ith entry in the list is the word tokenizaiton of the ith sentence (after preprocessing)","pos":43,"state":"done","type":"cell"}
{"cell_type":"code","id":"d037c3","input":"","pos":48,"state":"done","type":"cell"}
{"cell_type":"code","id":"d196db","input":"    ### Finding the most similar words in the model ###\n","pos":37,"state":"done","type":"cell"}
{"cell_type":"code","id":"e9c32a","input":"","pos":38,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"0f0f57","input":"## NLP Part 2 - Stopwords and Punctuation\n\nNow we are going to work to remove stopwords and punctuation from our data. Why do you think we are going to do this? Do some research if you don't know yet. \n\n","pos":12,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"15b811","input":"## Reflection\nHow important do you think proper preprocessing in NLP is?","pos":49,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"24b132","input":"## NLP Part 3b - Lemmatization\nLemmatization considers the context and converts the word to its meaningful base form. There is a cool tutorial and definition of lemmatization in NLTK [here](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/).","pos":20,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"3a289d","input":"### Why did we do all this work?","pos":41,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"4694c2","input":"## NLP Part 4 - POS Tagging\nParts of speech tagging is marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context.","pos":23,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"4758e4","input":"# Word2Vec Model Visualization","pos":26,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"4e414f","input":"# Natural Language Processing using NLTK","pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"699d72","input":"","pos":50,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"7f7bd6","input":"## NLP Part 1 - Tokenization of paragraphs/sentences\n\nIn this section we are going to tokenize our sentences and words. If you aren't familiar with tokenization, we recommend looking up \"what is tokenization\". \n\nYou should also spend time on the [NLTK documentation](https://www.nltk.org/). If you're not sure how to do something, or get an error, it is best to google it first and ask questions as you go!\n\n","pos":7,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"89d713","input":"","pos":1,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"9421a9","input":"## NLP Part 0 - Get some Data!\n\nThis section's code is mostly given to you as a review for how you can scrape and manipulate data from the web. ","pos":3,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"b4beae","input":"## Training the Word2Vec model\n\nFor this part you may want to follow a guide [here](https://radimrehurek.com/gensim/models/word2vec.html). \n\n","pos":30,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"cf87ca","input":"## Testing our model\n\n","pos":35,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"ffa5b8","input":"## NLP Part 3a - Stemming the words\nStemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. There is an example below!","pos":16,"state":"done","type":"cell"}
{"end":1656523691313,"exec_count":42,"id":"239552","input":"import nltk\nnltk.download('omw-1.4')","kernel":"nlp_env","output":{"0":{"name":"stderr","text":"[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n"},"1":{"data":{"text/plain":"True"},"exec_count":42}},"pos":20.5,"start":1656523691225,"state":"done","type":"cell"}
{"id":"c4e86f","input":"","pos":19.5,"type":"cell"}
{"id":0,"time":1656600034707,"type":"user"}
{"last_load":1656512391238,"type":"file"}