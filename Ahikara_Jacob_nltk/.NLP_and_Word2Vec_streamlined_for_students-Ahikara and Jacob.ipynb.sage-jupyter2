{"backend_state":"running","connection_file":"/projects/6a848d56-5da1-43fb-bfc9-9172977ceeb4/.local/share/jupyter/runtime/kernel-d4f211ea-cf84-43c5-a5ab-b6ee9118a323.json","kernel":"nlp_env","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"interpreter":{"hash":"335ee12212264728feb72f243af72c5a8ea26c832f07e1f651ce9e17c7ceae23"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1656599348921,"exec_count":1,"id":"2d58d8","input":"# Install NLTK - pip install nltk\nimport nltk\nnltk.download('wordnet')\nnltk.download('punkt')","kernel":"nlp_env","output":{"0":{"name":"stderr","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"},"1":{"data":{"text/plain":"True"},"exec_count":1}},"pos":2,"start":1656599348278,"state":"done","type":"cell"}
{"cell_type":"code","end":1656600000027,"exec_count":14,"id":"5a1173","input":"import urllib\nimport bs4 as bs\nimport re","kernel":"nlp_env","pos":4,"start":1656600000005,"state":"done","type":"cell"}
{"cell_type":"code","end":1656600008819,"exec_count":15,"id":"25dfda","input":"# We will read the contents of the Wikipedia article \"Global_warming\" as an example, please feel free to use your own! You can use the url below:\nurl = 'https://en.wikipedia.org/wiki/Solar_System' # you can change this to use other sites as well.\n\n# We can open the page using \"urllib.request.urlopen\" then read it using \".read()\"\nsource = urllib.request.urlopen(url).read()\n\n# Beautiful Soup is a Python library for pulling data out of HTML and XML files.\n# you may need to install a parser library --> \"!pip3 install lxml\"\n# Parsing the data/creating BeautifulSoup object\n\nsoup = bs.BeautifulSoup(source,\"html.parser\") \n\n# Fetching the data\ntext = \"\"\nfor paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n    text += paragraph.text\n\n# Preprocessing the data\n\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # [0-9]* --> Matches zero or more repetitions of any digit from 0 to 9\ntext = text.lower() #everything to lowercase\ntext = re.sub(r'\\W^.?!',' ',text) # \\W --> Matches any character which is not a word character except (.?!)\ntext = re.sub(r'\\d',' ',text) # \\d --> Matches any decimal digit\ntext = re.sub(r'\\s+',' ',text) # \\s --> Matches any characters that are considered whitespace (Ex: [\\t\\n\\r\\f\\v].)","kernel":"nlp_env","pos":5,"start":1656600007708,"state":"done","type":"cell"}
{"cell_type":"code","end":1656600014035,"exec_count":16,"id":"a4597c","input":"text[:100]","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"' the solar system[c] is the gravitationally bound system of the sun and the objects that orbit it. i'"},"exec_count":16}},"pos":6,"start":1656600014030,"state":"done","type":"cell"}
{"cell_type":"code","end":1656600162546,"exec_count":17,"id":"d58c3a","input":"'''\nYour code here: Tokenize the words from the data and set it to a variable called words.\nHint: how to this might be on the very home page of NLTK!\n'''\nwords = nltk.word_tokenize(text)","kernel":"nlp_env","pos":8,"start":1656600162481,"state":"done","type":"cell"}
{"cell_type":"code","end":1656600183838,"exec_count":18,"id":"0c950e","input":"print(words[:10])","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"['the', 'solar', 'system', '[', 'c', ']', 'is', 'the', 'gravitationally', 'bound']\n"}},"pos":9,"start":1656600183827,"state":"done","type":"cell"}
{"cell_type":"code","end":1656600254608,"exec_count":21,"id":"b8d41a","input":"'''\nYour code here: Tokenize the sentences from the data  and set it to a variable called sentences.\nHint: try googling how to tokenize sentences in NLTK!\n'''\nsentences = nltk.sent_tokenize(text)","kernel":"nlp_env","pos":10,"start":1656600254601,"state":"done","type":"cell"}
{"cell_type":"code","end":1656600258172,"exec_count":22,"id":"fff2e7","input":"print(sentences[:10])","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"[' the solar system[c] is the gravitationally bound system of the sun and the objects that orbit it.', 'it formed .', 'billion years ago from the gravitational collapse of a giant interstellar molecular cloud.', 'the vast majority ( .', \"%) of the system's mass is in the sun, with most of the remaining mass contained in the planet jupiter.\", 'the four inner system planets—mercury, venus, earth and mars—are terrestrial planets, being composed primarily of rock and metal.', 'the four giant planets of the outer system are substantially larger and more massive than the terrestrials.', 'the two largest, jupiter and saturn, are gas giants, being composed mainly of hydrogen and helium; the next two, uranus and neptune, are ice giants, being composed mostly of volatile substances with relatively high melting points compared with hydrogen and helium, such as water, ammonia, and methane.', \"all eight planets have nearly circular orbits that lie near the plane of earth's orbit, called the ecliptic.\", 'there are an unknown number of smaller dwarf planets and innumerable small solar system bodies orbiting the sun.']\n"}},"pos":11,"start":1656600258146,"state":"done","type":"cell"}
{"cell_type":"code","end":1656600282605,"exec_count":23,"id":"d45bdb","input":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords","kernel":"nlp_env","output":{"0":{"name":"stderr","text":"[nltk_data] Downloading package stopwords to\n[nltk_data]     /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"}},"pos":13,"start":1656600282597,"state":"done","type":"cell"}
{"cell_type":"code","end":1656600399246,"exec_count":24,"id":"d5e615","input":"'''\ndefine a function called \"remove_stopwords\" that takes in a list of the sentences of the text and returns one that doesn't have any stopwords.\n'''\ndef remove_stopwords(sentences):\n    \n    ### Some code goes here. Hint: You may have to look up how to remove stopwords in NLTK if you get stuck. ###\n    for i in range(len(sentences)):\n        # i = position of each sentence; i is a number\n        words = []\n        \n        word_list = nltk.word_tokenize(sentences[i])\n        \n        for word in word_list:\n            if word not in stopwords.words ('english'):\n                words.append(word)\n        \n        sentences[i] = ' '.join(words)\n        \n    return sentences\n\n###Then actually apply your function###\nsentences = remove_stopwords(sentences)\nprint(sentences[:10]) #Check if it worked correctly. Are all stopwords removed?","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"['solar system [ c ] gravitationally bound system sun objects orbit .', 'formed .', 'billion years ago gravitational collapse giant interstellar molecular cloud .', 'vast majority ( .', \"% ) system 's mass sun , remaining mass contained planet jupiter .\", 'four inner system planets—mercury , venus , earth mars—are terrestrial planets , composed primarily rock metal .', 'four giant planets outer system substantially larger massive terrestrials .', 'two largest , jupiter saturn , gas giants , composed mainly hydrogen helium ; next two , uranus neptune , ice giants , composed mostly volatile substances relatively high melting points compared hydrogen helium , water , ammonia , methane .', \"eight planets nearly circular orbits lie near plane earth 's orbit , called ecliptic .\", 'unknown number smaller dwarf planets innumerable small solar system bodies orbiting sun .']\n"}},"pos":14,"scrolled":true,"start":1656600398321,"state":"done","type":"cell"}
{"cell_type":"code","end":1656600846258,"exec_count":28,"id":"3fbf6d","input":"'''\nYour code here:\nDefine a function called \"stem_sentences\" that takes in a list of sentences and returns a list of stemmed sentences.\n'''\ndef stem_sentences(sentences):\n    ### Some code goes here. Hint: Try looking up how to stem words in NLTK if you get stuck (or simply use the example above and run stemmer in a loop!). ###\n    for i in range(len(sentences)):\n        # i = position of each sentence; i is a number\n        words = []\n        \n        word_list = nltk.word_tokenize(sentences[i])\n        \n        for word in word_list:\n            words.append(stemmer.stem(word))\n        \n        sentences[i] = ' '.join(words)\n        \n    return sentences\nsentences = stem_sentences(sentences)\nprint(sentences[:10]) # eliminating all stems from every word that is not in base form","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"['solar system [ c ] gravit bound system sun object orbit', 'form', 'billion year ago gravit collaps giant interstellar molecular cloud', 'vast major', \"% system 's mass sun remain mass contain planet jupit\", 'four inner system planets—mercuri venu earth mars—ar terrestri planet compos primarili rock metal', 'four giant planet outer system substanti larger massiv terrestri', 'two largest jupit saturn ga giant compos mainli hydrogen helium ; next two uranu neptun ice giant compos mostli volatil substanc rel high melt point compar hydrogen helium water ammonia methan', \"eight planet nearli circular orbit lie near plane earth 's orbit call eclipt\", 'unknown number smaller dwarf planet innumer small solar system bodi orbit sun']\n"}},"pos":18,"start":1656600846104,"state":"done","type":"cell"}
{"cell_type":"code","end":1656600869033,"exec_count":30,"id":"9e820e","input":"print(sentences[:10])","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"['solar system [ c ] gravit bound system sun object orbit', 'form', 'billion year ago gravit collaps giant interstellar molecular cloud', 'vast major', \"% system 's mass sun remain mass contain planet jupit\", 'four inner system planets—mercuri venu earth mars—ar terrestri planet compos primarili rock metal', 'four giant planet outer system substanti larger massiv terrestri', 'two largest jupit saturn ga giant compos mainli hydrogen helium ; next two uranu neptun ice giant compos mostli volatil substanc rel high melt point compar hydrogen helium water ammonia methan', \"eight planet nearli circular orbit lie near plane earth 's orbit call eclipt\", 'unknown number smaller dwarf planet innumer small solar system bodi orbit sun']\n"}},"pos":19,"start":1656600869030,"state":"done","type":"cell"}
{"cell_type":"code","end":1656601035849,"exec_count":32,"id":"c42133","input":"print(sentences[:10]) ","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"['solar system [ c ] gravit bound system sun object orbit', 'form', 'billion year ago gravit collaps giant interstellar molecular cloud', 'vast major', \"% system 's mass sun remain mass contain planet jupit\", 'four inner system planets—mercuri venu earth mars—ar terrestri planet compos primarili rock metal', 'four giant planet outer system substanti larger massiv terrestri', 'two largest jupit saturn ga giant compos mainli hydrogen helium ; next two uranu neptun ice giant compos mostli volatil substanc rel high melt point compar hydrogen helium water ammonia methan', \"eight planet nearli circular orbit lie near plane earth 's orbit call eclipt\", 'unknown number smaller dwarf planet innumer small solar system bodi orbit sun']\n"}},"pos":22,"start":1656601035845,"state":"done","type":"cell"}
{"cell_type":"code","end":1656601077363,"exec_count":33,"id":"73853f","input":"nltk.download('averaged_perceptron_tagger')","kernel":"nlp_env","output":{"0":{"name":"stderr","text":"[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"},"1":{"data":{"text/plain":"True"},"exec_count":33}},"pos":24,"start":1656601077349,"state":"done","type":"cell"}
{"cell_type":"code","end":1656601129359,"exec_count":36,"id":"c4cece","input":"# POS Tagging example\n# CC - coordinating conjunction\n# NN - noun, singular (cat, tree)\nall_words = nltk.word_tokenize(text)  ###If we want to look at part of speech taking before we stem/lem\n\ntagged_words = nltk.pos_tag(all_words)\n##Creates a list of lists where each element of the list is [word,partofspeech abbreviation]\n\n# Tagged word paragraph\nword_tags = []\nfor tw in tagged_words:\n    word_tags.append(tw[0]+\"_\"+tw[1])\n\ntagged_paragraph = ' '.join(word_tags)\n\n'''\nYour code here: print the first 1000 characters of tagged_paragraph.\n'''\ntagged_paragraph[:1000]","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"\"the_DT solar_JJ system_NN [_NNP c_NN ]_NN is_VBZ the_DT gravitationally_RB bound_JJ system_NN of_IN the_DT sun_NN and_CC the_DT objects_NNS that_WDT orbit_VBP it_PRP ._. it_PRP formed_VBD ._. billion_CD years_NNS ago_RB from_IN the_DT gravitational_JJ collapse_NN of_IN a_DT giant_JJ interstellar_JJ molecular_JJ cloud_NN ._. the_DT vast_JJ majority_NN (_( ._. %_NN )_) of_IN the_DT system_NN 's_POS mass_NN is_VBZ in_IN the_DT sun_NN ,_, with_IN most_JJS of_IN the_DT remaining_VBG mass_NN contained_VBN in_IN the_DT planet_NN jupiter_NN ._. the_DT four_CD inner_NN system_NN planets—mercury_NN ,_, venus_NN ,_, earth_NN and_CC mars—are_NN terrestrial_JJ planets_NNS ,_, being_VBG composed_VBN primarily_RB of_IN rock_NN and_CC metal_NN ._. the_DT four_CD giant_JJ planets_NNS of_IN the_DT outer_NN system_NN are_VBP substantially_RB larger_JJR and_CC more_RBR massive_JJ than_IN the_DT terrestrials_NNS ._. the_DT two_CD largest_JJS ,_, jupiter_NN and_CC saturn_NN ,_, are_VBP gas_NN giants_NNS ,_,\""},"exec_count":36}},"pos":25,"start":1656601128911,"state":"done","type":"cell"}
{"cell_type":"code","end":1656601140637,"exec_count":37,"id":"634986","input":"# Install gensim - pip install gensim\nimport nltk\nfrom gensim.models import Word2Vec\nimport matplotlib.pyplot as plt\nnltk.download('punkt')\nfrom wordcloud import WordCloud","kernel":"nlp_env","output":{"0":{"name":"stderr","text":"[nltk_data] Downloading package punkt to /projects/6a848d56-5da1-43fb-\n[nltk_data]     bfc9-9172977ceeb4/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"}},"pos":27,"start":1656601139818,"state":"done","type":"cell"}
{"cell_type":"code","end":1656601375851,"exec_count":41,"id":"7b6d05","input":"#Let's go ahead and create a list that's formatted how word2vec needs:\n    # a list of lists where the ith entry in the list is the word tokenization of the ith sentence (after preprocessing)\ntokenized = []\n\nfor sentence in sentences:\n    tokenized.append(nltk.word_tokenize(sentence))","kernel":"nlp_env","pos":28,"start":1656601375811,"state":"done","type":"cell"}
{"cell_type":"code","end":1656601382496,"exec_count":42,"id":"69c615","input":"# print the tokenized list of lists\ntokenized[:10]","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"[['solar',\n  'system',\n  '[',\n  'c',\n  ']',\n  'gravit',\n  'bound',\n  'system',\n  'sun',\n  'object',\n  'orbit'],\n ['form'],\n ['billion',\n  'year',\n  'ago',\n  'gravit',\n  'collaps',\n  'giant',\n  'interstellar',\n  'molecular',\n  'cloud'],\n ['vast', 'major'],\n ['%',\n  'system',\n  \"'s\",\n  'mass',\n  'sun',\n  'remain',\n  'mass',\n  'contain',\n  'planet',\n  'jupit'],\n ['four',\n  'inner',\n  'system',\n  'planets—mercuri',\n  'venu',\n  'earth',\n  'mars—ar',\n  'terrestri',\n  'planet',\n  'compos',\n  'primarili',\n  'rock',\n  'metal'],\n ['four',\n  'giant',\n  'planet',\n  'outer',\n  'system',\n  'substanti',\n  'larger',\n  'massiv',\n  'terrestri'],\n ['two',\n  'largest',\n  'jupit',\n  'saturn',\n  'ga',\n  'giant',\n  'compos',\n  'mainli',\n  'hydrogen',\n  'helium',\n  ';',\n  'next',\n  'two',\n  'uranu',\n  'neptun',\n  'ice',\n  'giant',\n  'compos',\n  'mostli',\n  'volatil',\n  'substanc',\n  'rel',\n  'high',\n  'melt',\n  'point',\n  'compar',\n  'hydrogen',\n  'helium',\n  'water',\n  'ammonia',\n  'methan'],\n ['eight',\n  'planet',\n  'nearli',\n  'circular',\n  'orbit',\n  'lie',\n  'near',\n  'plane',\n  'earth',\n  \"'s\",\n  'orbit',\n  'call',\n  'eclipt'],\n ['unknown',\n  'number',\n  'smaller',\n  'dwarf',\n  'planet',\n  'innumer',\n  'small',\n  'solar',\n  'system',\n  'bodi',\n  'orbit',\n  'sun']]"},"exec_count":42}},"pos":29,"start":1656601382478,"state":"done","type":"cell"}
{"cell_type":"code","end":1656601511915,"exec_count":44,"id":"2db988","input":"''' Training the Word2Vec model. You should pass:\n1. a list of lists where the ith entry in the list is the word tokenizaiton of the ith sentence\n2. min_count=1 --> Ignores all words with total frequency lower than 1 (i.e., include everything).\n'''\n# create the model\nmodel = Word2Vec(tokenized, min_count=1)\n\n# get the most common words of the model (it's entire vocabulary)\nmost_common_words = model.wv.index_to_key\n\n# save the model to use it later\nmodel.save(\"word2vec.model\")\n\n# model = Word2Vec.load(\"word2vec.model\")","kernel":"nlp_env","pos":31,"start":1656601511851,"state":"done","type":"cell"}
{"cell_type":"code","end":1656601514170,"exec_count":45,"id":"de74f7","input":"#print the first 10 most common words.\nmost_common_words[:10]","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"['sun',\n 'planet',\n 'system',\n 'solar',\n 'orbit',\n \"'s\",\n 'object',\n 'au',\n ';',\n 'belt']"},"exec_count":45}},"pos":32,"start":1656601514165,"state":"done","type":"cell"}
{"cell_type":"code","end":1656601625918,"exec_count":47,"id":"16ccfd","input":"model.wv.most_similar('planet')","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"[(\"'s\", 0.6326823234558105),\n ('sun', 0.6270672082901001),\n ('solar', 0.6227244734764099),\n ('system', 0.6065926551818848),\n ('orbit', 0.6057976484298706),\n ('``', 0.602124035358429),\n ('comet', 0.5705369114875793),\n ('creat', 0.5671244263648987),\n ('known', 0.5511392951011658),\n ('mass', 0.5510634779930115)]"},"exec_count":47}},"pos":34,"start":1656601625909,"state":"done","type":"cell"}
{"cell_type":"code","end":1656601669480,"exec_count":49,"id":"d936ab","input":"    # Finding Word Vectors - print word vectors for certain words in your text\nvector = model.wv['planet']\nprint(vector)","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"[-1.5128465e-02  1.1608394e-02  7.3302579e-03  3.2547119e-03\n  7.8544319e-03 -2.0531531e-02  3.9364831e-03  2.4511388e-02\n -9.1952840e-03 -8.7463697e-03 -4.7430517e-03 -1.9228622e-02\n -6.2649720e-03  9.2042787e-03  6.0813790e-03  1.9221344e-03\n  6.1771148e-03 -7.0404192e-04 -6.0376492e-03 -1.7986486e-02\n  5.2223024e-03 -1.9915923e-03  1.1086015e-02 -1.2467754e-02\n  4.7098347e-03  3.3772083e-03 -1.0624319e-02 -2.9271655e-03\n -1.0391565e-02  8.5536847e-03  2.1145998e-02 -8.7895029e-04\n  1.5726471e-03 -9.1704773e-03 -1.3407731e-05  1.2724730e-02\n  1.1006199e-02 -1.2712035e-03  6.1578071e-03 -6.2377476e-03\n  9.3280878e-03 -1.8075701e-02 -1.0818752e-02 -2.7444414e-03\n  3.8092041e-03  6.3946447e-03  3.1514994e-03 -2.7698940e-03\n  3.6011131e-03  4.5301500e-03  1.4809912e-02 -1.7501976e-02\n -2.6081540e-03  2.2135968e-03 -8.7033855e-03  1.2203348e-02\n  1.3635290e-02  6.0444493e-03 -1.0940971e-02  8.0556115e-03\n -5.6755426e-03  6.4500794e-03 -5.2500479e-03 -8.8737868e-03\n -6.0605113e-03  1.3422166e-02  1.4127823e-02  1.9618869e-03\n -5.7325037e-03  1.8600538e-02 -8.7284390e-03 -7.6431185e-03\n  1.3906020e-02  7.2151991e-03  7.1507925e-03 -1.0064443e-03\n -8.9947125e-03 -5.4158824e-03 -1.2047153e-03  1.2079215e-03\n -7.8528654e-03  2.3050345e-03 -3.8602115e-03  3.0939509e-03\n -9.4280654e-04 -2.9994778e-03  1.9084532e-03 -9.1007602e-04\n  1.2231223e-02 -4.5494894e-03  1.1338871e-02  5.8815931e-03\n  2.9001497e-03 -5.7505057e-03  1.3499531e-02  1.1053067e-02\n  1.2457099e-02 -1.2433594e-02 -6.7114914e-03  5.3770229e-04]\n"}},"pos":36,"start":1656601669472,"state":"done","type":"cell"}
{"cell_type":"code","end":1656602179648,"exec_count":53,"id":"a5ea8f","input":"# code to print a wordcloud for your sentences\nwordcloud = WordCloud(\n                        background_color='pink',\n                        max_words=100,\n                        max_font_size=50, \n                        random_state=42\n                        ).generate(str(sentences))\nfig = plt.figure(1)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"<Figure size 432x288 with 0 Axes>"}},"1":{"data":{"image/png":"78c5ef6483fd14a523e1eab7152ed9d764317b5f","text/plain":"<Figure size 720x720 with 1 Axes>"},"metadata":{"needs_background":"light"}}},"pos":40,"start":1656602179378,"state":"done","type":"cell"}
{"cell_type":"code","end":1656602222850,"exec_count":55,"id":"726a95","input":"# reFetching the data\nlame_text = \"\"\nfor paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n    lame_text += paragraph.text","kernel":"nlp_env","pos":42,"start":1656602222834,"state":"done","type":"cell"}
{"cell_type":"code","end":1656602236846,"exec_count":57,"id":"1017fc","input":"# Redo the word cloud but set stopwords to empty so it looks really bad\nwordcloud = WordCloud(\n                        background_color='white',\n                        max_words=100,\n                        max_font_size=50, \n                        random_state=42, ###SET STOPWORDS = [] and/or include_numbers = True or you will get the same thing!!!\n                        stopwords = [],\n                        include_numbers = True).generate(str(lame_sentences)) \nfig = plt.figure(1)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","kernel":"nlp_env","output":{"0":{"ename":"NameError","evalue":"name 'lame_sentences' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Redo the word cloud but set stopwords to empty so it looks really bad\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m WordCloud(\n\u001b[1;32m      3\u001b[0m                         background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                         max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      5\u001b[0m                         max_font_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, \n\u001b[1;32m      6\u001b[0m                         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, \u001b[38;5;66;03m###SET STOPWORDS = [] and/or include_numbers = True or you will get the same thing!!!\u001b[39;00m\n\u001b[1;32m      7\u001b[0m                         stopwords \u001b[38;5;241m=\u001b[39m [],\n\u001b[0;32m----> 8\u001b[0m                         include_numbers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;28mstr\u001b[39m(\u001b[43mlame_sentences\u001b[49m)) \n\u001b[1;32m      9\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n","\u001b[0;31mNameError\u001b[0m: name 'lame_sentences' is not defined"]}},"pos":44,"start":1656602236822,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"194b53","input":"### Finding the most similar words in the model but... you get the idea ###\n\n","pos":47,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"19b6f8","input":"    ### Finding the most similar words in the model ###\n","pos":37,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"1d38b9","input":"","pos":38,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"43aad4","input":"similar1, similar2","pos":39,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"5bc692","input":"# Look up the most similar words to certain words in your text using the model.wv.most_similar() function","pos":33,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"6a3e0e","input":"","pos":48,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"8fb16e","input":"'''\nDoing the same without removing stop words or lemming\n'''\n# tokenize the text using sent_tokenize\n\n# from this list of sentences, create a list of lists where the ith entry in the list is the word tokenizaiton of the ith sentence (after preprocessing)","pos":43,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"a9b022","input":"# Finding a vector of a word, but badly","pos":46,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"d5763b","input":"# Training the Word2Vec model (same code as before), but one change: use our lame data that was not preprocessed\n\n# Try printing this after training the model.\nwords = model.wv.index_to_key\nprint(words[:10])","pos":45,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"bb8871","input":"'''\ndefine a function called \"remove_punctuation\" that removes punctuation from the sentences.\n'''\ndef remove_punctuation(sentences):\n    \n    ### Some code goes here. Hint: Try looking up how to remove stopwords in NLTK if you get stuck. ###\n    for i in range(len(sentences)):\n        # i = position of each sentence; i is a number\n        words = []\n\n        word_list = nltk.word_tokenize(sentences[i])\n\n        for word in word_list:\n            if word not in \",.?/''()\":\n                words.append(word)\n\n        sentences[i] = ' '.join(words)\n\n    return sentences\nsentences = remove_punctuation(sentences)\nprint(sentences[:10]) #eliminating all punctuation.","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"['solar system [ c ] gravitationally bound system sun objects orbit', 'formed', 'billion years ago gravitational collapse giant interstellar molecular cloud', 'vast majority', \"% system 's mass sun remaining mass contained planet jupiter\", 'four inner system planets—mercury venus earth mars—are terrestrial planets composed primarily rock metal', 'four giant planets outer system substantially larger massive terrestrials', 'two largest jupiter saturn gas giants composed mainly hydrogen helium ; next two uranus neptune ice giants composed mostly volatile substances relatively high melting points compared hydrogen helium water ammonia methane', \"eight planets nearly circular orbits lie near plane earth 's orbit called ecliptic\", 'unknown number smaller dwarf planets innumerable small solar system bodies orbiting sun']\n"}},"pos":15,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":27,"id":"56dcc2","input":"from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n# try each of the words below\nstemmer.stem('troubled')\nstemmer.stem('trouble')\nstemmer.stem('troubling')\nstemmer.stem('troubles')","kernel":"nlp_env","output":{"0":{"data":{"text/plain":"'troubl'"},"exec_count":27}},"pos":17,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":31,"id":"64cf96","input":"from nltk.stem import WordNetLemmatizer\n    \n## Step 1: Import the lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n'''\nYour code here: Define a function called \"lem_sentences\" that: loops through the sentences, split the sentences up by words and applies \"lemmatizer.lemmatize\" to each word and then join everything back into a sentence\n'''\n##Similar to stopwords: For loop through the sentences, split by words and apply \"lemmatizer.lemmatize\" to each word and join back into a sentence\ndef lem_sentences(sentences):\n    \n    for i in range(len(sentences)):\n        words = nltk.word_tokenize(sentences[i])\n        \n        words = [lemmatizer.lemmatize(word) for word in words]\n        \n        sentences[i] = ' '.join(words)\n        \n    return sentences\nsentences = lem_sentences(sentences)\nprint(sentences[:10]) ","kernel":"nlp_env","output":{"0":{"name":"stdout","text":"['solar system [ c ] gravit bound system sun object orbit', 'form', 'billion year ago gravit collaps giant interstellar molecular cloud', 'vast major', \"% system 's mass sun remain mass contain planet jupit\", 'four inner system planets—mercuri venu earth mars—ar terrestri planet compos primarili rock metal', 'four giant planet outer system substanti larger massiv terrestri', 'two largest jupit saturn ga giant compos mainli hydrogen helium ; next two uranu neptun ice giant compos mostli volatil substanc rel high melt point compar hydrogen helium water ammonia methan', \"eight planet nearli circular orbit lie near plane earth 's orbit call eclipt\", 'unknown number smaller dwarf planet innumer small solar system bodi orbit sun']\n"}},"pos":21,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"02f5b1","input":"## Training the Word2Vec model\n\nFor this part you may want to follow a guide [here](https://radimrehurek.com/gensim/models/word2vec.html). ","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"0bdb41","input":"## Testing our model","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"1422be","input":"# Word2Vec Model Visualization","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"1e86bb","input":"## NLP Part 3a - Stemming the words\nStemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. There is an example below!","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"5ad9d3","input":"","pos":50,"type":"cell"}
{"cell_type":"markdown","id":"73671a","input":"## NLP Part 2 - Stopwords and Punctuation\nNow we are going to work to remove stopwords and punctuation from our data. Why do you think we are going to do this? Do some research if you don't know yet. ","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"930c9a","input":"## Reflection\nHow important do you think proper preprocessing in NLP is?","pos":49,"type":"cell"}
{"cell_type":"markdown","id":"963e4b","input":"### Why did we do all this work?","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"97cf90","input":"## NLP Part 0 - Get some Data!\n\nThis section's code is mostly given to you as a review for how you can scrape and manipulate data from the web. ","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"acd900","input":"","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"ae807e","input":"## NLP Part 3b - Lemmatization\n\nLemmatization considers the context and converts the word to its meaningful base form. There is a cool tutorial and definition of lemmatization in NLTK [here](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/).\n\n","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"ead2bf","input":"## NLP Part 1 - Tokenization of paragraphs/sentences\n\nIn this section we are going to tokenize our sentences and words. If you aren't familiar with tokenization, we recommend looking up \"what is tokenization\". \n\nYou should also spend time on the [NLTK documentation](https://www.nltk.org/). If you're not sure how to do something, or get an error, it is best to google it first and ask questions as you go!\n\n","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"ed341f","input":"## NLP Part 4 - POS Tagging\nParts of speech tagging is marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context.","pos":23,"type":"cell"}
{"cell_type":"markdown","id":"ff4f91","input":"# Natural Language Processing using NLTK","pos":0,"type":"cell"}
{"id":0,"time":1656601476874,"type":"user"}
{"last_load":1656599312260,"type":"file"}